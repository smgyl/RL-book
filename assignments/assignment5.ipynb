{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/mac/Downloads/RL-book-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Mapping, Tuple, TypeVar, Sequence, List\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Distribution, Constant\n",
    "from rl.function_approx import FunctionApprox\n",
    "from rl.iterate import iterate\n",
    "from rl.markov_process import (FiniteMarkovRewardProcess, MarkovRewardProcess,\n",
    "                               RewardTransition)\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess, Policy,\n",
    "                                        MarkovDecisionProcess,\n",
    "                                        StateActionMapping)\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    approx_0: FunctionApprox[S],\n",
    "    non_terminal_states_distribution: Distribution[S],\n",
    "    num_state_samples: int\n",
    ") -> Iterator[Tuple[FunctionApprox[S], Policy[S, A]]]:\n",
    "    def greedy_policy_from_v(\n",
    "        v: FunctionApprox[S],\n",
    "    ) -> Policy[S, A]:\n",
    "        def return_(s_r: Tuple[S, float]) -> float:\n",
    "            s1, r = s_r\n",
    "            return r + gamma * v.evaluate([s1]).item()\n",
    "        class my_policy(Policy[S, A]):\n",
    "            def act(self, s: S) -> Optional[Distribution[A]]:\n",
    "                if mdp.is_terminal(s):\n",
    "                    return None\n",
    "                q_values = ((a, mdp.step(s, a).expectation(return_)) for a in mdp.actions(s))\n",
    "                action = max(q_values, key=operator.itemgetter(1))[0]\n",
    "                return Constant(action)\n",
    "        return my_policy()\n",
    "    def update(vf_policy: Tuple[FunctionApprox[S], Policy[S, A]])-> Tuple[FunctionApprox[S], Policy[S, A]]:\n",
    "\n",
    "        vf, pi = vf_policy\n",
    "        mrp: MarkovRewardProcess[S] = mdp.apply_policy(pi)\n",
    "        policy_vf: FunctionApprox[S] = last(evaluate_mrp(mrp, gamma, vf, non_terminal, num_state))\n",
    "        improved_pi: Policy[S, A] = greedy_policy_from_v(policy_vf)\n",
    "\n",
    "        return policy_vf, improved_pi\n",
    "\n",
    "    pi_0 = greedy_policy_from_v(approx_0)\n",
    "    return iterate(update, (approx_0, pi_0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
